{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKMrEGZtgyFp",
        "outputId": "9335c70b-b743-49e2-b97c-28df8f6a6926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking runtime configuration...\n",
            "GPU Runtime: Tesla T4\n",
            "CUDA Version: 12.6\n",
            "GPU Memory: 15.8 GB\n",
            "Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Runtime check complete!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"Checking runtime configuration...\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Runtime: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"GPU NOT AVAILABLE!\")\n",
        "    print(\"To fix: Runtime -> Change runtime type -> Hardware accelerator -> GPU\")\n",
        "import sys\n",
        "print(f\"Python Version: {sys.version}\")\n",
        "print(\"Runtime check complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elKGnUbqgyFt",
        "outputId": "46042629-266d-4c4f-c90f-d1edd6ea1bfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing system dependencies...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Installing Python packages...\n",
            "Dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "print(\"Installing system dependencies...\")\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq ffmpeg libsm6 libxext6 libxrender-dev libglib2.0-0\n",
        "print(\"Installing Python packages...\")\n",
        "!pip install -q \"numpy>=2.0.0,<3.0.0\"\n",
        "!pip install -q \"protobuf>=5.26.1,<7.0.0\"\n",
        "!pip install -q pyngrok\n",
        "print(\"Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYFrGtfygyFu",
        "outputId": "db91a585-2826-4e2d-f38c-9721eb419ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning repository: https://github.com/bitcoin4cashqc/Deep-Live-Cam-Server.git\n",
            "Branch: Server\n",
            "Cloning into 'Deep-Live-Cam-Server'...\n",
            "remote: Enumerating objects: 1563, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 1563 (delta 25), reused 20 (delta 10), pack-reused 1519 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1563/1563), 152.12 MiB | 20.50 MiB/s, done.\n",
            "Resolving deltas: 100% (888/888), done.\n",
            "Repository cloned to: /content/Deep-Live-Cam-Server\n",
            "Server\n",
            "Found: server_ws.py\n",
            "Found: client_ws.py\n",
            "Found: requirements.txt\n",
            "Found: modules/websocket_server.py\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "# UPDATE THIS URL\n",
        "REPO_URL = \"https://github.com/bitcoin4cashqc/Deep-Live-Cam-Server.git\"\n",
        "BRANCH = \"Server\"\n",
        "print(f\"Cloning repository: {REPO_URL}\")\n",
        "print(f\"Branch: {BRANCH}\")\n",
        "project_dir = \"/content/Deep-Live-Cam-Server\"\n",
        "if os.path.exists(project_dir):\n",
        "    shutil.rmtree(project_dir)\n",
        "os.chdir('/content')\n",
        "!git clone -b {BRANCH} {REPO_URL}\n",
        "os.chdir(project_dir)\n",
        "print(f\"Repository cloned to: {os.getcwd()}\")\n",
        "!git branch --show-current\n",
        "key_files = ['server_ws.py', 'client_ws.py', 'requirements.txt', 'modules/websocket_server.py']\n",
        "for file in key_files:\n",
        "    if os.path.exists(file):\n",
        "        print(f\"Found: {file}\")\n",
        "    else:\n",
        "        print(f\"Missing: {file}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading required models...\")\n",
        "models_dir = \"models\"\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "# Download inswapper model\n",
        "print(\"Downloading inswapper_128_fp16.onnx...\")\n",
        "!wget -q -O models/inswapper_128_fp16.onnx \"https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx?download=true\"\n",
        "\n",
        "# Download GFPGAN model\n",
        "print(\"Downloading GFPGANv1.4.pth...\")\n",
        "!wget -q -O models/GFPGANv1.4.pth \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/GFPGANv1.4.pth\"\n",
        "\n",
        "print(\"Models downloaded successfully!\")\n",
        "print(f\"Models saved in: {os.path.join(os.getcwd(), models_dir)}\")\n",
        "\n",
        "# Verify downloads\n",
        "model_files = [\"inswapper_128_fp16.onnx\", \"GFPGANv1.4.pth\"]\n",
        "for model_file in model_files:\n",
        "    model_path = os.path.join(models_dir, model_file)\n",
        "    if os.path.exists(model_path):\n",
        "        size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
        "        print(f\"✓ {model_file}: {size_mb:.1f} MB\")\n",
        "    else:\n",
        "        print(f\"✗ {model_file}: NOT FOUND\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LrUm8RLgyFw",
        "outputId": "6fc6a969-fc70-491b-9ba5-f28f263f4871"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading required models...\n",
            "Downloading inswapper_128_fp16.onnx...\n",
            "Downloading GFPGANv1.4.pth...\n",
            "Models downloaded successfully!\n",
            "Models saved in: /content/Deep-Live-Cam-Server/models\n",
            "✓ inswapper_128_fp16.onnx: 264.4 MB\n",
            "✓ GFPGANv1.4.pth: 332.5 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dTApCUZgyFy",
        "outputId": "b333f2bb-95ab-4205-ea55-62009d4c9178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing project dependencies...\n",
            "Installing PyTorch with CUDA...\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "GPU ready: Tesla T4\n",
            "Dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing project dependencies...\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Installing PyTorch with CUDA...\")\n",
        "    !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "    !pip install -q onnxruntime-gpu\n",
        "else:\n",
        "    print(\"Installing PyTorch CPU...\")\n",
        "    !pip install -q torch torchvision torchaudio\n",
        "    !pip install -q onnxruntime\n",
        "!pip install -q opencv-python==4.10.0.84\n",
        "!pip install -q \"websockets>=11.0\"\n",
        "!pip install -q insightface==0.7.3\n",
        "!pip install -q \"pillow>=11.0.0\"\n",
        "!pip install -q psutil\n",
        "!pip install -q opennsfw2\n",
        "!pip install -q \"git+https://github.com/xinntao/BasicSR.git@master\"\n",
        "!pip install -q \"git+https://github.com/TencentARC/GFPGAN.git@master\"\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU ready: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "print(\"Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rRzuiTvgyF0",
        "outputId": "65d23814-d5e7-4db3-fbb4-7c53abc75337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok tunnel created: tcp://6.tcp.ngrok.io:16814\n",
            "WebSocket URL: ws://6.tcp.ngrok.io:16814\n",
            "Use this in clients: python client_ws.py -s face.jpg --server-url ws://6.tcp.ngrok.io:16814\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok, conf\n",
        "# ADD YOUR TOKEN HERE\n",
        "NGROK_TOKEN = \"6oL7zUZ6WPjQoTCnar3Yt_7ATyJfwNHKZw4Kh9uMZNA\"\n",
        "if NGROK_TOKEN == \"YOUR_NGROK_TOKEN_HERE\":\n",
        "    print(\"Add your ngrok token above!\")\n",
        "    print(\"Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "    websocket_url = \"ws://localhost:8765\"\n",
        "    print(f\"Using localhost: {websocket_url}\")\n",
        "else:\n",
        "    conf.get_default().auth_token = NGROK_TOKEN\n",
        "    ngrok.kill()\n",
        "    try:\n",
        "        tunnel = ngrok.connect(8765, \"http\")\n",
        "        tunnel_url = tunnel.public_url\n",
        "        websocket_url = f\"ws://{tunnel_url.replace('http://', '')}\"\n",
        "        print(f\"Ngrok tunnel created: {tunnel_url}\")\n",
        "        print(f\"WebSocket URL: {websocket_url}\")\n",
        "        print(f\"Use this in clients: python client_ws.py -s face.jpg --server-url {websocket_url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ngrok failed: {e}\")\n",
        "        websocket_url = \"ws://localhost:8765\"\n",
        "        print(f\"Using localhost: {websocket_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import subprocess\n",
        "import sys\n",
        "print(\"Starting Deep Live Cam WebSocket Server...\")\n",
        "print(f\"WebSocket URL: {websocket_url}\")\n",
        "print(f\"GPU: {'Available' if torch.cuda.is_available() else 'CPU Only'}\")\n",
        "print(f\"Directory: {os.getcwd()}\")\n",
        "print(\"Client examples:\")\n",
        "print(f\"  python client_ws.py -s alice.jpg --server-url {websocket_url}\")\n",
        "print(f\"  python client_ws.py -s bob.jpg --server-url {websocket_url}\")\n",
        "cmd = [sys.executable, \"server_ws.py\", \"--headless\"]\n",
        "if torch.cuda.is_available():\n",
        "    cmd.extend([\"--execution-provider\", \"cuda\", \"--execution-threads\", \"4\"])\n",
        "    print(\"Using CUDA\")\n",
        "else:\n",
        "    cmd.extend([\"--execution-provider\", \"cpu\", \"--execution-threads\", \"2\"])\n",
        "    print(\"Using CPU\")\n",
        "cmd.extend([\"--server-port\", \"8765\"])\n",
        "print(f\"Starting: {' '.join(cmd)}\")\n",
        "print(\"-\" * 50)\n",
        "try:\n",
        "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True, bufsize=1)\n",
        "    print(\"Server started! Press Interrupt to stop.\")\n",
        "    for line in process.stdout:\n",
        "        print(line.strip())\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Server stopped by user\")\n",
        "    process.terminate()\n",
        "except Exception as e:\n",
        "    print(f\"Server error: {e}\")\n",
        "finally:\n",
        "    try:\n",
        "        ngrok.kill()\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSBBbYr4keun",
        "outputId": "6b5883ef-4c48-41cd-d38c-228c80f3100c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Deep Live Cam WebSocket Server...\n",
            "WebSocket URL: ws://6.tcp.ngrok.io:16814\n",
            "GPU: Available\n",
            "Directory: /content/Deep-Live-Cam-Server\n",
            "Client examples:\n",
            "  python client_ws.py -s alice.jpg --server-url ws://6.tcp.ngrok.io:16814\n",
            "  python client_ws.py -s bob.jpg --server-url ws://6.tcp.ngrok.io:16814\n",
            "Using CUDA\n",
            "Starting: /usr/bin/python3 server_ws.py --headless --execution-provider cuda --execution-threads 4 --server-port 8765\n",
            "--------------------------------------------------\n",
            "Server started! Press Interrupt to stop.\n",
            "2025-10-15 06:36:50.311431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760510210.345531    9367 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760510210.356738    9367 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760510210.390098    9367 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760510210.390201    9367 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760510210.390220    9367 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760510210.390236    9367 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Deep Live Cam - WebSocket Server\n",
            "==================================\n",
            "Server starting - clients will provide their own source faces\n",
            "Port: custom\n",
            "Execution Provider: custom\n",
            "Each client can connect with their own face to swap\n",
            "[DLC.CORE] Starting WebSocket server...\n",
            "\n",
            "Downloading:   0%|          | 0.00/49.7k [00:00<?, ?B/s]\n",
            "Downloading:  16%|█▌        | 8.00k/49.7k [00:00<00:00, 59.7kB/s]\n",
            "Downloading: 64.0kB [00:00, 456kB/s]\n",
            "\u001b[0;93m2025-10-15 06:36:58.212091209 [W:onnxruntime:, graph.cc:4859 CleanUnusedInitializersAndNodeArgs] Removing initializer 'buff2fs'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "INFO:modules.websocket_server:Starting WebSocket server on port 8765\n",
            "INFO:websockets.server:server listening on 0.0.0.0:8765\n",
            "INFO:modules.websocket_server:WebSocket server started on ws://0.0.0.0:8765\n",
            "Server stopped by user\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erUJYahDgyF1",
        "outputId": "c08c51b5-b9eb-40fe-8fbe-21bba1f8fab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test failed: did not receive a valid HTTP response\n",
            "Server not responding\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import websockets\n",
        "import json\n",
        "async def test_connection():\n",
        "    try:\n",
        "        ws = await websockets.connect(\"ws://localhost:8765\")\n",
        "        await ws.send(json.dumps({'type': 'stats_request'}))\n",
        "        response = await asyncio.wait_for(ws.recv(), timeout=5)\n",
        "        data = json.loads(response)\n",
        "        await ws.close()\n",
        "        if data.get('type') == 'stats':\n",
        "            stats = data.get('data', {})\n",
        "            print(f\"Server running! Clients: {stats.get('connected_clients', 0)}\")\n",
        "            return True\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"Test failed: {e}\")\n",
        "        return False\n",
        "result = await test_connection()\n",
        "if result:\n",
        "    print(f\"Server ready at: {websocket_url}\")\n",
        "else:\n",
        "    print(\"Server not responding\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}